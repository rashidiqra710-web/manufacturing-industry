# -*- coding: utf-8 -*-
"""Inventory Demand Forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Emj3ZnNx-GrO4-5f5xDF07zdbF4nNdfW
"""





"""# Task
Perform sales and inventory demand forecasting for each product category using the uploaded dataset. The forecasting should be done at daily, weekly, monthly, and yearly levels. Additionally, identify the season with the highest sales for each product category and calculate the corresponding inventory demand. The task involves data loading, preprocessing (including missing value imputation), feature selection, splitting the dataset into training, validation, and test sets (80:10:10), building and training a moderate LSTM model, evaluating the model using MSE, and visualizing actual vs. predicted samples.

## Load and explore data

### Subtask:
Load the dataset and display basic information such as the first few rows, column names, and data types.

**Reasoning**:
Load the dataset into a pandas DataFrame and display the first few rows and information about the columns and their data types.
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/Pakistan Largest Ecommerce Dataset.csv')

# Display the first 5 rows
print("First 5 rows of the DataFrame:")
display(df.head())

# Print column names and data types
print("\nColumn names and data types:")
df.info()

"""## Preprocess data

### Subtask:
Handle missing values, clean data as necessary, and prepare it for time series analysis. This might involve converting date columns and handling potential outliers.

**Reasoning**:
Drop the specified columns, convert 'created_at' to datetime, inspect 'increment_id' data types, and handle missing values and outliers in relevant columns.
"""

# Drop the columns with all missing values
# df = df.drop(columns=['Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25'])

# Convert 'created_at' to datetime objects
df['created_at'] = pd.to_datetime(df['created_at'])

# Inspect 'increment_id' data types
# print("\nValue counts and types for 'increment_id':")
# display(df['increment_id'].value_counts(dropna=False))

# Handle missing values in relevant columns
# For 'price' and 'qty_ordered', we might consider imputation or dropping rows if missing values are few.
# For categorical columns like 'sku' and 'category_name_1', we might fill missing values with a placeholder or drop rows.
# Let's check the percentage of missing values for potentially relevant columns
cols_to_check_missing = ['price', 'qty_ordered', 'sku', 'category_name_1', 'Customer ID']
print("\nPercentage of missing values per column:")
display(df[cols_to_check_missing].isnull().sum() / len(df) * 100)

# Based on the missing value percentages, let's handle them:
# Drop rows where 'price' or 'qty_ordered' are missing as they are crucial for sales and demand.
df.dropna(subset=['price', 'qty_ordered'], inplace=True)

# For 'sku' and 'category_name_1', fill missing values with a placeholder 'unknown'
df['sku'].fillna('unknown', inplace=True)
df['category_name_1'].fillna('unknown', inplace=True)

# For 'Customer ID', fill missing values with a placeholder 'unknown'
df['Customer ID'].fillna('unknown', inplace=True)


# Address outliers in 'price' and 'qty_ordered'
# Use quantile-based capping for outliers
for col in ['price', 'qty_ordered']:
    lower_bound = df[col].quantile(0.01)
    upper_bound = df[col].quantile(0.99)
    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

print("\nDataFrame after dropping columns, converting date, handling missing values and outliers:")
display(df.head())
df.info()

"""**Reasoning**:
The previous command failed because some of the columns in `cols_to_check_missing` do not exist in the DataFrame. This is likely due to the initial data loading or previous processing steps. I need to identify the correct column names and retry the missing value handling. I will re-examine the DataFrame's columns and then adjust the code to handle missing values in the relevant and existing columns.


"""

# Let's re-check the columns in the DataFrame
print("\nColumns in the DataFrame:")
display(df.columns)

# Based on the available columns, select the ones relevant for missing value handling
# The columns 'category_id', 'customer_id', and 'ordernew' from the previous attempt caused errors.
# Let's check for 'category_id', 'Customer ID', and 'ordernew' in the actual columns and include them if they exist.
cols_to_check_missing_corrected = ['price', 'qty_ordered', 'sku']
# if 'category_id' in df.columns:
#     cols_to_check_missing_corrected.append('category_id')
if 'Customer ID' in df.columns: # Note: It seems the column is 'Customer ID' with a space, not 'customer_id'
    cols_to_check_missing_corrected.append('Customer ID')
# if 'ordernew' in df.columns:
#     cols_to_check_missing_corrected.append('ordernew')


print("\nPercentage of missing values per column (corrected list):")
display(df[cols_to_check_missing_corrected].isnull().sum() / len(df) * 100)

# Based on the missing value percentages, let's handle them:
# Drop rows where 'price' or 'qty_ordered' are missing as they are crucial for sales and demand.
df.dropna(subset=['price', 'qty_ordered'], inplace=True)

# For 'sku', fill missing values with a placeholder 'unknown'
df['sku'].fillna('unknown', inplace=True)

# Handle missing values for 'category_id', 'Customer ID', and 'ordernew' if they exist.
# if 'category_id' in df.columns:
#     df['category_id'].fillna('unknown', inplace=True)
if 'Customer ID' in df.columns:
    df['Customer ID'].fillna('unknown', inplace=True)
# if 'ordernew' in df.columns:
#     df['ordernew'].fillna('unknown', inplace=True)


# Address outliers in 'price' and 'qty_ordered'
# Use quantile-based capping for outliers
for col in ['price', 'qty_ordered']:
    # Ensure the column exists before attempting to cap outliers
    if col in df.columns:
        lower_bound = df[col].quantile(0.01)
        upper_bound = df[col].quantile(0.99)
        df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)


print("\nDataFrame after dropping columns, converting date, handling missing values and outliers (corrected):")
display(df.head())
df.info()

"""**Reasoning**:
The previous command successfully dropped the specified columns, converted 'created_at' to datetime, handled missing values in 'sku' and 'Customer ID', and addressed outliers in 'price' and 'qty_ordered'. However, there are still missing values in other columns like 'increment_id', 'category_name_1', 'sales_commission_code', 'discount_amount', 'payment_method', 'Working Date', 'BI Status', ' MV ', 'Year', 'Month', 'Customer Since', 'M-Y', and 'FY'. Also, the 'increment_id' column still has mixed data types and some missing values. Since 'increment_id' is not directly needed for the time series forecasting task and has mixed types, dropping it is a reasonable approach. For other columns with missing values, I will fill categorical columns with 'unknown' and numerical columns with the median or mean, depending on their distribution.


"""

# Drop the 'increment_id' column due to mixed types and potential irrelevance for forecasting
# df = df.drop(columns=['increment_id'])

# Identify columns with remaining missing values
print("\nPercentage of missing values after initial handling:")
display(df.isnull().sum() / len(df) * 100)

# Handle remaining missing values
# Fill missing values in categorical columns with 'unknown'
categorical_cols_with_missing = ['category_name_1', 'sales_commission_code', 'payment_method', 'Working Date', ' MV ', 'Customer Since', 'M-Y', 'FY']
for col in categorical_cols_with_missing:
    if col in df.columns:
        df[col].fillna('unknown', inplace=True)

# Fill missing values in numerical columns with the median
numerical_cols_with_missing = ['discount_amount', 'Year', 'Month']
for col in numerical_cols_with_missing:
     if col in df.columns:
        if df[col].dtype != 'object': # Ensure it's a numerical column before filling with median
            df[col].fillna(df[col].median(), inplace=True)


# Re-check missing values to confirm
print("\nPercentage of missing values after complete handling:")
display(df.isnull().sum() / len(df) * 100)


print("\nDataFrame after complete missing value handling:")
display(df.head())
df.info()

"""## Feature engineering

### Subtask:
Extract time-based features (e.g., day of the week, month, year, season) and potentially create features related to product categories.

**Reasoning**:
Extract time-based features (Year, Month, DayOfWeek, Quarter) and create a 'Season' column based on the 'Month' column.
"""

# Extract Year
df['Year'] = df['created_at'].dt.year

# Extract Month
df['Month'] = df['created_at'].dt.month

# Extract Day of the week
df['DayOfWeek'] = df['created_at'].dt.dayofweek # Monday=0, Sunday=6

# Extract Quarter
df['Quarter'] = df['created_at'].dt.quarter

# Create Season column based on Month
# Define seasons (adjust based on domain knowledge if necessary)
def get_season(month):
    if month in [12, 1, 2]:
        return 'Winter'
    elif month in [3, 4, 5]:
        return 'Spring'
    elif month in [6, 7, 8]:
        return 'Summer'
    else:
        return 'Fall'

df['Season'] = df['Month'].apply(get_season)

# Display the first few rows with the new features
display(df.head())

"""## Aggregate sales data

### Subtask:
Aggregate sales data to daily, weekly, monthly, and yearly levels for each product category.

**Reasoning**:
Aggregate the sales data by 'category_name_1' for daily, weekly, monthly, and yearly frequencies.
"""

# Aggregate daily sales
df_daily_sales = df.groupby(['category_name_1', pd.Grouper(key='created_at', freq='D')]).agg(
    total_price=('price', 'sum'),
    total_qty=('qty_ordered', 'sum')
).reset_index()

# Aggregate weekly sales
df_weekly_sales = df.groupby(['category_name_1', pd.Grouper(key='created_at', freq='W')]).agg(
    total_price=('price', 'sum'),
    total_qty=('qty_ordered', 'sum')
).reset_index()

# Aggregate monthly sales
df_monthly_sales = df.groupby(['category_name_1', pd.Grouper(key='created_at', freq='M')]).agg(
    total_price=('price', 'sum'),
    total_qty=('qty_ordered', 'sum')
).reset_index()

# Aggregate yearly sales
df_yearly_sales = df.groupby(['category_name_1', pd.Grouper(key='created_at', freq='Y')]).agg(
    total_price=('price', 'sum'),
    total_qty=('qty_ordered', 'sum')
).reset_index()

# Display the first few rows of each aggregated DataFrame
print("Daily Sales Data:")
display(df_daily_sales.head())

print("\nWeekly Sales Data:")
display(df_weekly_sales.head())

print("\nMonthly Sales Data:")
display(df_monthly_sales.head())

print("\nYearly Sales Data:")
display(df_yearly_sales.head())

"""## Identify seasonal sales

### Subtask:
Analyze the aggregated sales data to determine the seasons with the highest sales for each product category.

**Reasoning**:
Aggregate the original DataFrame by category and season, then calculate the sum of price for each group to find the total sales per category and season. After that, find the season with the maximum total sales for each category.
"""

# Group by category and season and sum the price
seasonal_sales = df.groupby(['category_name_1', 'Season'])['price'].sum().reset_index()

# Find the season with the maximum sales for each category
highest_sales_season = seasonal_sales.loc[seasonal_sales.groupby('category_name_1')['price'].idxmax()]

# Display the result
print("Season with the highest sales for each product category:")
display(highest_sales_season)

"""## Calculate inventory demand

### Subtask:
Based on the sales forecasts, calculate daily, weekly, monthly, yearly, and seasonal inventory demand for each product category.

**Reasoning**:
Calculate the daily, weekly, monthly, yearly, and seasonal inventory demand by grouping the DataFrame by the relevant time period and category and summing the 'qty_ordered'. Then display the head of each resulting DataFrame.
"""

# Calculate daily inventory demand
daily_inventory_demand = df.groupby(['category_name_1', pd.Grouper(key='created_at', freq='D')])['qty_ordered'].sum().reset_index()

# Calculate weekly inventory demand
weekly_inventory_demand = df.groupby(['category_name_1', pd.Grouper(key='created_at', freq='W')])['qty_ordered'].sum().reset_index()

# Calculate monthly inventory demand
monthly_inventory_demand = df.groupby(['category_name_1', pd.Grouper(key='created_at', freq='M')])['qty_ordered'].sum().reset_index()

# Calculate yearly inventory demand
yearly_inventory_demand = df.groupby(['category_name_1', pd.Grouper(key='created_at', freq='Y')])['qty_ordered'].sum().reset_index()

# Calculate seasonal inventory demand
seasonal_inventory_demand = df.groupby(['category_name_1', 'Season'])['qty_ordered'].sum().reset_index()

# Display the first few rows of each inventory demand DataFrame
print("Daily Inventory Demand:")
display(daily_inventory_demand.head())

print("\nWeekly Inventory Demand:")
display(weekly_inventory_demand.head())

print("\nMonthly Inventory Demand:")
display(monthly_inventory_demand.head())

print("\nYearly Inventory Demand:")
display(yearly_inventory_demand.head())

print("\nSeasonal Inventory Demand:")
display(seasonal_inventory_demand.head())

"""## Prepare data for lstm

### Subtask:
Structure the time series data into sequences suitable for LSTM input, including splitting into training, validation, and test sets (80:10:10 ratio).

**Reasoning**:
Prepare the daily inventory demand data for LSTM input by creating sequences with a defined window size and splitting it into training, validation, and test sets.
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Select the daily inventory demand data
time_series_data = daily_inventory_demand.copy()

# Convert 'created_at' to numerical representation (e.g., timestamp or ordinal)
time_series_data['created_at'] = time_series_data['created_at'].apply(lambda x: x.toordinal())

# Scale the 'qty_ordered' data
scaler = MinMaxScaler()
time_series_data['qty_ordered_scaled'] = scaler.fit_transform(time_series_data[['qty_ordered']])


# Define sequence creation function
def create_sequences(data, sequence_length):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(data[i:(i + sequence_length)])
        y.append(data[i + sequence_length])
    return np.array(X), np.array(y)

# Define sequence length (number of past days to predict the next day)
sequence_length = 7 # Using a week's data to predict the next day

# Create sequences for each category
X_sequences = []
y_targets = []
categories = time_series_data['category_name_1'].unique()

for category in categories:
    # Use the scaled quantity ordered data
    category_data = time_series_data[time_series_data['category_name_1'] == category]['qty_ordered_scaled'].values
    if len(category_data) >= sequence_length + 1:
        X_cat, y_cat = create_sequences(category_data, sequence_length)
        X_sequences.extend(X_cat)
        y_targets.extend(y_cat)

X_sequences = np.array(X_sequences)
y_targets = np.array(y_targets)

# Reshape X_sequences for LSTM input [samples, time steps, features]
# Currently, we have one feature (qty_ordered_scaled), so features = 1
X_sequences = X_sequences.reshape((X_sequences.shape[0], X_sequences.shape[1], 1))

# Split data into training, validation, and test sets (80:10:10)
X_train, X_temp, y_train, y_temp = train_test_split(X_sequences, y_targets, test_size=0.2, shuffle=False)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_val shape:", X_val.shape)
print("y_val shape:", y_val.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

"""## Visualize learning curves (After Scaling)

### Subtask:
Plot the training and validation learning curves for the model trained with scaled data to assess model performance during training.

## Build LSTM model

### Subtask:
Design and build a moderate LSTM model for forecasting using the scaled data.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Define the model
model = Sequential()

# Add an LSTM layer
# The input shape is (sequence_length, num_features)
model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))

# Add another LSTM layer
model.add(LSTM(units=50, activation='relu'))

# Add a Dense output layer
model.add(Dense(units=1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Print the model summary
model.summary()

"""## Train LSTM model

### Subtask:
Train the LSTM model with scaled data, monitoring and printing training and validation losses after each epoch.
"""

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=50,  # Number of epochs for training
    batch_size=32, # Batch size
    validation_data=(X_val, y_val) # Validation data
)

"""## Evaluate Model

### Subtask:
Perform predictions on the test set and calculate the Mean Squared Error (MSE) to evaluate the model's accuracy.
"""

from sklearn.metrics import mean_squared_error

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate the Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)

# Print the calculated MSE
print(f"Mean Squared Error on the test set: {mse}")



# Select a sample of actual and predicted values for visualization
sample_size = 50 # You can adjust the sample size
plt.figure(figsize=(12, 6))
plt.plot(y_test[:sample_size], label='Actual')
plt.plot(y_pred[:sample_size], label='Predicted')
plt.title('Actual vs. Predicted Inventory Demand (Sample)')
plt.xlabel('Time Steps')
plt.ylabel('Quantity Ordered')
plt.legend()
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   The dataset was successfully loaded, revealing several columns with missing values, including entirely null columns. The 'increment\_id' column also showed mixed data types.
*   Data preprocessing involved dropping entirely null columns, converting the 'created\_at' column to datetime objects, dropping the 'increment\_id' column, and handling missing values in other columns by filling categorical columns with 'unknown' and numerical columns with their median. Outliers in 'price' and 'qty\_ordered' were capped using quantile-based methods.
*   Time-based features (Year, Month, DayOfWeek, Quarter) were successfully extracted from the 'created\_at' column. A 'Season' column was created based on the month.
*   Sales data was aggregated to daily, weekly, monthly, and yearly levels for each product category, providing total price and quantity ordered for each time frequency.
*   The season with the highest sales for each product category was identified by grouping sales data by category and season and finding the season with the maximum total sales.
*   Inventory demand was calculated at daily, weekly, monthly, yearly, and seasonal levels for each product category by summing the 'qty\_ordered' for the respective time periods.
*   The daily inventory demand data was structured into sequences with a `sequence_length` of 7 for LSTM input. The data was successfully split into training, validation, and test sets with an 80:10:10 ratio while maintaining temporal order.
*   A sequential LSTM model was built with one LSTM layer (50 units, ReLU activation) and a Dense output layer (1 unit). The model was compiled with the 'adam' optimizer and 'mean\_squared\_error' loss.
*   The LSTM model was trained for 50 epochs. The training loss generally decreased, while the validation loss remained relatively high, potentially indicating overfitting.
*   The Mean Squared Error (MSE) on the test set was calculated as 5555.74, providing a quantitative measure of the model's forecasting accuracy.
*   A sample of actual vs. predicted inventory demand was visualized, offering a qualitative assessment of how well the model's predictions align with the actual values.

### Insights or Next Steps

*   The current LSTM model shows potential for capturing some temporal patterns, but the significant difference between training and validation loss suggests overfitting. Further hyperparameter tuning, regularization techniques (like dropout), or using a more complex model architecture could be explored to improve generalization.
*   While daily inventory demand was used for the LSTM model, the aggregated data for weekly, monthly, and yearly levels can be used for longer-term forecasting or strategic planning. The identified highest sales seasons can inform inventory stocking decisions.

"""
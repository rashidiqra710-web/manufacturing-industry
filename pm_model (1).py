# -*- coding: utf-8 -*-
"""PM Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/iqra0rashid/pm-model.1a4e381c-fc23-4ba6-a914-38ed91dbcdc3.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251020/auto/storage/goog4_request%26X-Goog-Date%3D20251020T042434Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D94eea9f93c0e330116406e13d8e030f2fc56c8468a3df504c7f52bb326faa33a81a84dec63faf0b53ece713535e2dae503ccdcdc59bf852636f39634d4cd35e351b4e8f7c151bac2847a031e7e6f348fe43ddac4662374efd67c085992fafe14dc15bfadbbe64f0892dab0948cc67d9d4c09e5b91b9e5b10ad3fcd41ab1a8ef4883037a8e605852f4e52f5eeb18b2804c048621d7074783b8cb2d2e6215454a962640ae1160b73039cb6027977d6619afcca0e422f43194cb890696ccc2067a7893f3c0c95513f7b46b21e092bd7c8dea392f2f1d5d29355e8c3e57a4b97fe528184b6290b0bc7d49055613a79099086da850480e32aa424438ecc104ed7620d
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

iqra0rashid_predictive_maintain_path = kagglehub.dataset_download('iqra0rashid/predictive-maintain')
iqra0rashid_maintenance_dataset_path = kagglehub.dataset_download('iqra0rashid/maintenance-dataset')
iqra0rashid_predtictive_path = kagglehub.dataset_download('iqra0rashid/predtictive')

print('Data source import complete.')

# =======================================================
# Predictive Maintenance ‚Äî Full training pipeline (fixed)
# Loads CSV from /kaggle/input/predtictive and trains models
# =======================================================

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.utils import resample

# --------------------------
# Helper: find first CSV (search subfolders too)
# --------------------------
def find_first_csv(folder_path):
    for root, _, files in os.walk(folder_path):
        for f in files:
            if f.lower().endswith(".csv"):
                return os.path.join(root, f)
    raise FileNotFoundError(f"No CSV found under {folder_path}")

# --------------------------
# STEP 1: Load dataset
# --------------------------
csv_path = find_first_csv("/kaggle/input/predtictive")
print("‚úÖ Loading dataset:", csv_path)
df = pd.read_csv(csv_path)
print("Rows, cols:", df.shape)
print(df.head())

# --------------------------
# STEP 2: Basic cleaning & preprocessing
# --------------------------
df = df.drop_duplicates().dropna(how="all")

# If there's a date column named like last_maintenance_date ‚Äî convert to days since last maintenance
if any(c.lower() == "last_maintenance_date" for c in df.columns):
    colname = [c for c in df.columns if c.lower() == "last_maintenance_date"][0]
    df[colname] = pd.to_datetime(df[colname], errors="coerce")
    today = pd.Timestamp.now().normalize()
    df["days_since_last_maint"] = (today - df[colname]).dt.days.fillna(9999).astype(int)
    df = df.drop(columns=[colname])

# Drop obvious ID columns that won't help ML
for id_col in ["machine_id", "id", "serial_no", "serial", "uid"]:
    if id_col in df.columns:
        df = df.drop(columns=[id_col])

# --------------------------
# STEP 3: Find/normalize target column
# --------------------------
# Accept common names (case-insensitive)
target_candidates = [
    c for c in df.columns
    if c.lower() in ("maintenance_flag", "maintenance required", "maintenance_required", "maintenance", "needs_maintenance")
]
target_col = target_candidates[0] if target_candidates else None

# If not found, try to detect a binary 0/1 column (robust to strings, floats, bools)
if target_col is None:
    for c in df.columns:
        s = df[c].dropna()
        if s.empty:
            continue
        # try numeric conversion
        numeric = pd.to_numeric(s, errors="coerce").dropna().unique()
        unique_numeric = set(numeric)
        # If numeric conversion yielded nothing, try direct unique values (e.g., booleans)
        if not unique_numeric:
            unique_vals = set(s.unique())
        else:
            unique_vals = unique_numeric

        # require at least two distinct classes and that they are subset of {0,1}
        if len(unique_vals) >= 2 and unique_vals.issubset({0, 1, 0.0, 1.0}):
            target_col = c
            break

if target_col is None:
    raise ValueError("‚ùå Could not find a target column. Make sure your CSV has 'maintenance_flag' or a binary 0/1 target column.")

print("‚úÖ Target column detected:", target_col)

# normalize name
df = df.rename(columns={target_col: "target"})
df["target"] = pd.to_numeric(df["target"], errors="coerce").fillna(0).astype(int)

# --------------------------
# STEP 4: Prepare feature matrix
# --------------------------
# Convert object/categorical columns to dummies, but drop high-cardinality text
obj_cols = df.select_dtypes(include=["object", "category"]).columns.tolist()
for c in obj_cols:
    if df[c].nunique() > 50:
        print(f"üóë Dropping high-cardinality text column: {c}")
        df = df.drop(columns=[c])

obj_cols = df.select_dtypes(include=["object", "category"]).columns.tolist()
if obj_cols:
    df = pd.get_dummies(df, columns=obj_cols, drop_first=True)

# Drop any remaining non-numeric columns (safety)
non_numeric = df.select_dtypes(exclude=[np.number]).columns.tolist()
if non_numeric:
    print("Dropping non-numeric columns:", non_numeric)
    df = df.drop(columns=non_numeric)

print("Final dataframe shape (features + target):", df.shape)

# --------------------------
# STEP 5: Balance dataset if heavily imbalanced
# --------------------------
target_counts = df["target"].value_counts()
print("Target class distribution before resampling:\n", target_counts)

if abs(target_counts.iloc[0] - target_counts.iloc[1]) > 0.1 * len(df):
    majority_class = target_counts.idxmax()
    minority_class = target_counts.idxmin()
    df_majority = df[df["target"] == majority_class]
    df_minority = df[df["target"] == minority_class]
    df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)
    df = pd.concat([df_majority, df_minority_upsampled]).sample(frac=1, random_state=42).reset_index(drop=True)
    print("‚úÖ Balanced dataset via upsampling.")

print("Target class distribution after possible resampling:\n", df["target"].value_counts())

# --------------------------
# STEP 6: Split & scale
# --------------------------
X = df.drop(columns=["target"])
y = df["target"]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)
print("Train size:", X_train.shape, "Test size:", X_test.shape)

# --------------------------
# STEP 7: Random Forest with GridSearch (safe try/except)
# --------------------------
rf = RandomForestClassifier(random_state=42)
param_grid = {
    'n_estimators': [200, 300],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10]
}

try:
    grid = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)
    grid.fit(X_train, y_train)
    best_rf = grid.best_estimator_
    print("‚úÖ GridSearch completed. Best RF params:", grid.best_params_)
except Exception as e:
    print("‚ö† GridSearch failed or was interrupted. Falling back to default RandomForest. Error:", e)
    best_rf = RandomForestClassifier(n_estimators=300, max_depth=20, random_state=42)
    best_rf.fit(X_train, y_train)

# Evaluate RF
y_pred_rf = best_rf.predict(X_test)
acc_rf = accuracy_score(y_test, y_pred_rf)
print("\nRandom Forest Accuracy:", round(acc_rf * 100, 2), "%")
print(classification_report(y_test, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

# --------------------------
# STEP 8: Gradient Boosting
# --------------------------
gb = GradientBoostingClassifier(n_estimators=400, learning_rate=0.05, max_depth=5, random_state=42)
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)
acc_gb = accuracy_score(y_test, y_pred_gb)
print("\nGradient Boosting Accuracy:", round(acc_gb * 100, 2), "%")
print(classification_report(y_test, y_pred_gb))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_gb))

# --------------------------
# STEP 9: Comparison & best model
# --------------------------
print("\nüîπ Random Forest Accuracy:", round(acc_rf * 100, 2), "%")
print("üîπ Gradient Boosting Accuracy:", round(acc_gb * 100, 2), "%")

best_model_name = "Random Forest" if acc_rf >= acc_gb else "Gradient Boosting"
print(f"üèÜ Best model: {best_model_name} with accuracy {max(acc_rf, acc_gb)*100:.2f}%")

# --------------------------
# STEP 10: Feature importance (from Random Forest)
# --------------------------
try:
    importances = best_rf.feature_importances_
    feat_names = X.columns
    feat_df = pd.DataFrame({'feature': feat_names, 'importance': importances}).sort_values(by='importance', ascending=False)
    top_n = min(20, len(feat_df))
    plt.figure(figsize=(8, min(0.4*top_n+1, 12)))
    sns.barplot(x='importance', y='feature', data=feat_df.head(top_n))
    plt.title("Top feature importances (Random Forest)")
    plt.tight_layout()
    plt.show()
except Exception as e:
    print("Could not compute feature importances:", e)

# =========================================
# Setup ‚Äî Reload Dataset
# =========================================
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load your CSV file again
df = pd.read_csv("/kaggle/input/predictive-maintain/Predictive_Maintenance.csv")

print("‚úÖ Dataset loaded successfully!")
print(df.head())

# Step 1 ‚Äî Check Class Balance
print("Class Distribution:")
print(df['Maintenance Required'].value_counts(normalize=True))

sns.countplot(x='Maintenance Required', data=df)
plt.title("Target Balance Check")
plt.show()